\chapter{Conclusions and Future Work}
\label{section:conclusions-and-future-work}

\label{Conclusions}
\label{section:conclusions}

3D reconstruction of real world scenes is an important field today with many applications. The most promising technology relies on LiDAR laser scanners and cameras to capture both the geometry and color information of the scene. However, the fusion of the data of both sensors is still in open development. This work presents a methodology for both the capture process, as well as the algorithms to reconstruct the geometry and the color information from the laser scanner and camera data. 

First, a mobile 3D scanner was developed to obtain the data required. This 3D scanner is composed, at its core, by a pan and tilt unit, a 3D laser scanner and a camera. In particular, three laser scanners were used, to test the validity of the reconstruction algorithms and also to study the influence of the laser scanner in the final result. Moreover, the laser scanner SICK LMS100 was found to be inadequate for this reconstruction, because the laser scans appear deformed, so the laser was ultimately not used subsequently. The reason for this inaccuracies was not found out.

The scene capturing methodology describes a method to minimize the technical limitations of the sensors, so that the scene capture is not influenced by them. Hence, each capture is composed by several acquisitions, that are taken from different poses in the scene. In each acquisition, the PTU joints moves though multiple waypoints and multiple images and laser scans are recorded during this movement. The pose and number of acquisitions should be adequate such that every part of the scene is recorded. However, this planning is done a priori, so it is not always certain that the scene is totally captured, as seen in the results.

The first reconstruction is the geometric reconstruction, which uses the laser scanner data and the PTU transformations to reconstruct the geometry, resulting in a point cloud. This reconstruction relies on an accurate calibration of the laser scanner to the PTU frame, which is called the extrinsic laser calibration. The first attempt of this calibration was with the RADLOCC calibration. However, because of the poor results obtained by this calibration, a new calibration method was developed, which has superior results. Furthermore, a normal estimation method was developed that exploits the bi-dimensional structure of the point cloud obtained to estimate the normals. This method is faster than the more standard method using the $k$-neighbors without sacrificing the overall result. However, this method is sensible to inconsistencies in the movement of the PTU, specially at the interruptions using to capture the images. Finally, the registration of multiple acquisitions was done using the ICP method. However, this method was sub-optimal, because the registration failed if the initial estimate of the transformations were not close to the correct transformation. To bypass this problem, the initial estimate was done manually first. Also, the ICP method is limited to a pairwise registration, so three methods were tried to apply the ICP to multiple point clouds. The method that better performed, is the registration against the accumulated point cloud. In sum, the ICP method has many shortcomings, and its use always require manual work. Overall, the geometric registration was successful and the resulting point cloud is dense and accurate. 

The next reconstruction if the color reconstruction, which uses the image taken from the camera to attribute colors to the point cloud obtained previously. This reconstruction was separated into two steps. First, each image is registered in the point cloud, by re-projection of the points in the image surface. Then, the points that are outside the visual frustum of the camera and the points that are hidden are removed. Finally, the color correspondent to each projected point is obtained using a bilinear interpolation. After this step, each image has a correspondent partial colorization of the point cloud. This step relies on the projection matrix and extrinsic transformation of the camera to register the points in the image, which are obtained via the intrinsic calibration and extrinsic camera calibration. Afterwards, each point has multiple correspondent colors associated, but an unique color has to be chosen for each point, which is done in the color fusion step. Multiple fusion techniques were tried, as choosing the first, last or closest color from all the registered colors, or by calculating the average color of the registered colors. Overall, the color reconstruction is not optimal due to several factors. First, the images vary in hue or intensity, due to factors during the capture, like the lightning of the scene. This creates inconsistencies in the colorization point cloud that are really noticeable in the final results. Secondly, the extrinsic of the camera is not accurate, so the color registration is imperfect. 

Overall, the results are satisfactory, specially the geometric registration. There are, however, numerous limitation on both registrations that require to be solved, for example, a better extrinsic camera calibration, or an automatic acquisition registration method.


\subsection{Future Work}
\label{section:future-work}

The methodologies explored in this work still leaves much room for exploration and development. Some algorithms and methods have shortcomings in their performance and usability. Hereby, some possible solutions and thoughts that could be explored and developed in the future work are described. In particular, possible solutions for the acquisition registration, color normalization and camera extrinsic calibration are discussed.

The acquisition registration can be improved in two approaches. The first approach could be the integration of some localization device in the robot, as for example a inertial measurement unit, which can be used to obtain an approximate transformation between the acquisitions. This approximate solution could replace the initial estimate done manually in this work. The second approach world be  to replace the ICP algorithm, because of its disadvantages. Firstly, the new algorithm should be able to register multiple point clouds at once, instead of just two. This algorithm world probably have a better accuracy than the methods used in this work. Also, the distance between point clouds, which is the heuristic of the ICP algorithms should be modified, because most times it is incorrect and is only successfully if the two point clouds are very close. Instead, features could be extracted from the point clouds, like edges and corners, and their distance should be used instead. Because this features are sparse, it is possible that this correspondence is more accurate.

Moreover, the images obtained should have a similar color for the same object, which is not the case, due to the different illumination. So, a normalization of the images should be done prior to any color registration phase. Also, the use of high dynamic range photography should be used to capture the enable this normalization. As a result, the colors of the same objects should be the same across all the images, and this color should be independent of the lightning of the scene.

Finally, a new calibration method to obtain the extrinsic calibration of the camera should be developed. The principal limitations of the hand-in-eye algorithms, is the fact that only a small portion of the angle interval of the joint angles are used for the calibration, and the ArUCO pose estimation has an high error. Recently, in the master thesis of Filipe Costa \cite{costa18}, a bundle calibration method of ArUCO markers and camera was developed. With small adjustments, this algorithm can be modified to calibrate one camera mounted on a PTU. More specifically, this calibration obtains a precise pose of the camera and all the ArUCO markers, which could be used to fed the original hand-in-eye algorithms, with the change that now there are multiple markers instead of just one. It is possible that this method could achieve better results that the original hand-in-eye method used in this work.
